# Environment-Aware Rendering and Interaction in Web Based Augmented Reality

- Web-based framework for environment-aware rendering and interaction in augmented reality based on WebXR.
- Realistic rendering of 3D objects that handles with geometry occlusions, matches the lighting of the environment, casts shadows, and provides physics interaction with real-world objects.
- Target the web environment and have designed our solution to work on a vast range of devices as, for example, monocular camera setups using depth data generated by deep neural networks (DNN) or devices with dedicated higher quality depth sensors (e.g. LIDAR, structured light, stereo depth) that provides a more accurate interaction with the environment.
- Capable of obtaining over 20 frames per second even on middle-range devices.
- Depth data provided by WebXR to dynamically place 3D objects around the environment, calculate geometry occlusion on a pixel basis directly from the projected vertices coordinates to prevent loss in data precision and builds a voxelized model of the environment using a probabilistic grid approach to enable physics interaction.
- Consistency in the rendering of the virtual scene we implement a physically-based rendering pipeline in which physically correct attributes are associated with each 3D object that, combined with lighting information captured by the device, enables the rendering of AR content that matches the real-world illumination.



### Getting Started



### Algorithms



### License

The code from the project is MIT licensed. The license is available on the project repository,